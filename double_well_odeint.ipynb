{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import argparse\n",
    "\n",
    "\n",
    "NONLINEARITIES = {\n",
    "    \"tanh\": nn.Tanh(),\n",
    "    \"relu\": nn.ReLU(),\n",
    "    \"softplus\": nn.Softplus(),\n",
    "    \"elu\": nn.ELU(),\n",
    "}\n",
    "\n",
    "class ConcatSquashConv2d(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\n",
    "        super(ConcatSquashConv2d, self).__init__()\n",
    "        module = nn.ConvTranspose2d if transpose else nn.Conv2d\n",
    "        self._layer = module(\n",
    "            dim_in, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n",
    "            bias=bias\n",
    "        )\n",
    "        self._hyper_gate = nn.Linear(1, dim_out)\n",
    "        self._hyper_bias = nn.Linear(1, dim_out, bias=False)\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        return self._layer(x) * torch.sigmoid(self._hyper_gate(t.view(1, 1))).view(1, -1, 1, 1) \\\n",
    "            + self._hyper_bias(t.view(1, 1)).view(1, -1, 1, 1)\n",
    "\n",
    "def squeeze(input, downscale_factor=2):\n",
    "    '''\n",
    "    [:, C, H*r, W*r] -> [:, C*r^2, H, W]\n",
    "    '''\n",
    "    batch_size, in_channels, in_height, in_width = input.size()\n",
    "    out_channels = in_channels * (downscale_factor**2)\n",
    "\n",
    "    out_height = in_height // downscale_factor\n",
    "    out_width = in_width // downscale_factor\n",
    "\n",
    "    input_view = input.contiguous().view(\n",
    "        batch_size, in_channels, out_height, downscale_factor, out_width, downscale_factor\n",
    "    )\n",
    "\n",
    "    output = input_view.permute(0, 1, 3, 5, 2, 4).contiguous()\n",
    "    return output.view(batch_size, out_channels, out_height, out_width)\n",
    "\n",
    "def unsqueeze(input, upscale_factor=2):\n",
    "    '''\n",
    "    [:, C*r^2, H, W] -> [:, C, H*r, W*r]\n",
    "    '''\n",
    "    batch_size, in_channels, in_height, in_width = input.size()\n",
    "    out_channels = in_channels // (upscale_factor**2)\n",
    "\n",
    "    out_height = in_height * upscale_factor\n",
    "    out_width = in_width * upscale_factor\n",
    "\n",
    "    input_view = input.contiguous().view(batch_size, out_channels, upscale_factor, upscale_factor, in_height, in_width)\n",
    "\n",
    "    output = input_view.permute(0, 1, 4, 2, 5, 3).contiguous()\n",
    "    return output.view(batch_size, out_channels, out_height, out_width)\n",
    "\n",
    "class ODEnet(nn.Module):\n",
    "    \"\"\"\n",
    "    Helper class to make neural nets for use in continuous normalizing flows\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, hidden_dims, input_shape, strides, conv, layer_type=\"concat\", nonlinearity=\"softplus\", num_squeeze=0\n",
    "    ):\n",
    "        super(ODEnet, self).__init__()\n",
    "        self.num_squeeze = num_squeeze\n",
    "       \n",
    "        strides = [None] * (len(hidden_dims) + 1)\n",
    "        \n",
    "        base_layer=ConcatSquashConv2d\n",
    "        # build layers and add them\n",
    "        layers = []\n",
    "        activation_fns = []\n",
    "        hidden_shape = input_shape\n",
    "\n",
    "        for dim_out, stride in zip(hidden_dims + (input_shape[0],), strides):\n",
    "            if stride is None:\n",
    "                layer_kwargs = {}\n",
    "            elif stride == 1:\n",
    "                layer_kwargs = {\"ksize\": 3, \"stride\": 1, \"padding\": 1, \"transpose\": False}\n",
    "            elif stride == 2:\n",
    "                layer_kwargs = {\"ksize\": 4, \"stride\": 2, \"padding\": 1, \"transpose\": False}\n",
    "            elif stride == -2:\n",
    "                layer_kwargs = {\"ksize\": 4, \"stride\": 2, \"padding\": 1, \"transpose\": True}\n",
    "            else:\n",
    "                raise ValueError('Unsupported stride: {}'.format(stride))\n",
    "\n",
    "            layer = base_layer(hidden_shape[0], dim_out, **layer_kwargs)\n",
    "            layers.append(layer)\n",
    "            activation_fns.append(NONLINEARITIES[nonlinearity])\n",
    "\n",
    "            hidden_shape = list(copy.copy(hidden_shape))\n",
    "            hidden_shape[0] = dim_out\n",
    "            if stride == 2:\n",
    "                hidden_shape[1], hidden_shape[2] = hidden_shape[1] // 2, hidden_shape[2] // 2\n",
    "            elif stride == -2:\n",
    "                hidden_shape[1], hidden_shape[2] = hidden_shape[1] * 2, hidden_shape[2] * 2\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.activation_fns = nn.ModuleList(activation_fns[:-1])\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        dx = y\n",
    "        # squeeze\n",
    "        for _ in range(self.num_squeeze):\n",
    "            dx = squeeze(dx, 2)\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            dx = layer(t, dx)\n",
    "            # if not last layer, use nonlinearity\n",
    "            if l < len(self.layers) - 1:\n",
    "                dx = self.activation_fns[l](dx)\n",
    "        # unsqueeze\n",
    "        for _ in range(self.num_squeeze):\n",
    "            dx = unsqueeze(dx, 2)\n",
    "        return dx\n",
    "    \n",
    "def build_model_tabular(args, dims, regularization_fns=None):\n",
    "\n",
    "    hidden_dims = tuple(map(int, args.dims.split(\"-\")))\n",
    "\n",
    "    def build_cnf():\n",
    "        diffeq = ODEnet(\n",
    "            hidden_dims=hidden_dims,\n",
    "            input_shape=(dims,),\n",
    "            strides=None,\n",
    "            conv=False,\n",
    "            layer_type=args.layer_type,\n",
    "            nonlinearity=args.nonlinearity,\n",
    "        )\n",
    "        odefunc = layers.ODEfunc(\n",
    "            diffeq=diffeq,\n",
    "            divergence_fn=args.divergence_fn,\n",
    "            residual=args.residual,\n",
    "            rademacher=args.rademacher,\n",
    "        )\n",
    "        cnf = layers.CNF(\n",
    "            odefunc=odefunc,\n",
    "            T=args.time_length,\n",
    "            train_T=args.train_T,\n",
    "            regularization_fns=regularization_fns,\n",
    "            solver=args.solver,\n",
    "        )\n",
    "        return cnf\n",
    "\n",
    "    chain = [build_cnf() for _ in range(args.num_blocks)]\n",
    "    if args.batch_norm:\n",
    "        bn_layers = [layers.MovingBatchNorm1d(dims, bn_lag=args.bn_lag) for _ in range(args.num_blocks)]\n",
    "        bn_chain = [layers.MovingBatchNorm1d(dims, bn_lag=args.bn_lag)]\n",
    "        for a, b in zip(chain, bn_layers):\n",
    "            bn_chain.append(a)\n",
    "            bn_chain.append(b)\n",
    "        chain = bn_chain\n",
    "    model = layers.SequentialFlow(chain)\n",
    "\n",
    "    set_cnf_options(args, model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Continuous Normalizing Flow [-h]\n",
      "                                   [--data {swissroll,8gaussians,pinwheel,circles,moons,2spirals,checkerboard,rings}]\n",
      "                                   [--layer_type {ignore,concat,concat_v2,squash,concatsquash,concatcoord,hyper,blend}]\n",
      "                                   [--dims DIMS] [--num_blocks NUM_BLOCKS]\n",
      "                                   [--time_length TIME_LENGTH]\n",
      "                                   [--train_T TRAIN_T]\n",
      "                                   [--divergence_fn {brute_force,approximate}]\n",
      "                                   [--nonlinearity {tanh,relu,softplus,elu}]\n",
      "                                   [--solver {dopri5,bdf,rk4,midpoint,adams,explicit_adams,fixed_adams}]\n",
      "                                   [--atol ATOL] [--rtol RTOL]\n",
      "                                   [--step_size STEP_SIZE]\n",
      "                                   [--test_solver {dopri5,bdf,rk4,midpoint,adams,explicit_adams,fixed_adams,None}]\n",
      "                                   [--test_atol TEST_ATOL]\n",
      "                                   [--test_rtol TEST_RTOL]\n",
      "                                   [--residual {True,False}]\n",
      "                                   [--rademacher {True,False}]\n",
      "                                   [--spectral_norm {True,False}]\n",
      "                                   [--batch_norm {True,False}]\n",
      "                                   [--bn_lag BN_LAG] [--niters NITERS]\n",
      "                                   [--batch_size BATCH_SIZE]\n",
      "                                   [--test_batch_size TEST_BATCH_SIZE]\n",
      "                                   [--lr LR] [--weight_decay WEIGHT_DECAY]\n",
      "                                   [--l1int L1INT] [--l2int L2INT]\n",
      "                                   [--dl2int DL2INT] [--JFrobint JFROBINT]\n",
      "                                   [--JdiagFrobint JDIAGFROBINT]\n",
      "                                   [--JoffdiagFrobint JOFFDIAGFROBINT]\n",
      "                                   [--save SAVE] [--viz_freq VIZ_FREQ]\n",
      "                                   [--val_freq VAL_FREQ] [--log_freq LOG_FREQ]\n",
      "                                   [--gpu GPU]\n",
      "Continuous Normalizing Flow: error: unrecognized arguments: --f=/home/jingyang/.local/share/jupyter/runtime/kernel-v31781b894521c165aa8224c94f8197ca75118fddb.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams', 'fixed_adams']\n",
    "parser = argparse.ArgumentParser('Continuous Normalizing Flow')\n",
    "parser.add_argument(\n",
    "    '--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],\n",
    "    type=str, default='pinwheel'\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--layer_type\", type=str, default=\"concatsquash\",\n",
    "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
    ")\n",
    "parser.add_argument('--dims', type=str, default='64-64-64')\n",
    "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
    "parser.add_argument('--time_length', type=float, default=0.5)\n",
    "parser.add_argument('--train_T', type=eval, default=True)\n",
    "parser.add_argument(\"--divergence_fn\", type=str, default=\"brute_force\", choices=[\"brute_force\", \"approximate\"])\n",
    "parser.add_argument(\"--nonlinearity\", type=str, default=\"tanh\", choices=NONLINEARITIES)\n",
    "\n",
    "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
    "parser.add_argument('--atol', type=float, default=1e-5)\n",
    "parser.add_argument('--rtol', type=float, default=1e-5)\n",
    "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
    "\n",
    "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
    "parser.add_argument('--test_atol', type=float, default=None)\n",
    "parser.add_argument('--test_rtol', type=float, default=None)\n",
    "\n",
    "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
    "parser.add_argument('--rademacher', type=eval, default=False, choices=[True, False])\n",
    "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
    "parser.add_argument('--batch_norm', type=eval, default=False, choices=[True, False])\n",
    "parser.add_argument('--bn_lag', type=float, default=0)\n",
    "\n",
    "parser.add_argument('--niters', type=int, default=10000)\n",
    "parser.add_argument('--batch_size', type=int, default=100)\n",
    "parser.add_argument('--test_batch_size', type=int, default=1000)\n",
    "parser.add_argument('--lr', type=float, default=1e-3)\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-5)\n",
    "\n",
    "# Track quantities\n",
    "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
    "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
    "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
    "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
    "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
    "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
    "\n",
    "parser.add_argument('--save', type=str, default='experiments/cnf')\n",
    "parser.add_argument('--viz_freq', type=int, default=100)\n",
    "parser.add_argument('--val_freq', type=int, default=100)\n",
    "parser.add_argument('--log_freq', type=int, default=10)\n",
    "parser.add_argument('--gpu', type=int, default=0)\n",
    "args = parser.parse_args()\n",
    "device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = build_model_tabular(args, dims=2).to(device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
